[TRAINING]
num_epochs = 10000
eval_batch_size = 200
eval_every = 200
learning_rate = 0.001
checkpoints_to_keep = 2
save_every = 100000
log_device_placement = False
patience = 5
dropout = 0.2

[DATA]
logs_path = logs
data_path = /corpora
train_file = train.tsv
dev_file = dev.tsv
test_file = test.tsv
train_bpe_file = train.bpe.tsv
dev_bpe_file = dev.bpe.tsv
test_bpe_file = test.bpe.tsv
vocab_file = vocab.txt
vocabulary_size = 1000
max_sequence_len=71
embeddings=emb.npy

[PARAMS]
embedding_size = 300
emb_vocab_size = 50000
hidden_size=256
